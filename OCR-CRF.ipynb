{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import itertools\n",
    "import math\n",
    "import scipy.misc\n",
    "\n",
    "\n",
    "TRAIN_DIR = os.path.join(os.getcwd(),\"Assignment2\\\\data\\\\train\")\n",
    "TEST_DIR = os.path.join(os.getcwd(),\"Assignment2\\\\data\\\\test\")\n",
    "MODEL_DIR= os.path.join(os.getcwd(),\"Assignment2\\\\model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenames=os.listdir(MODEL_DIR)\n",
    "\n",
    "feature_gradient=np.loadtxt(os.path.join(MODEL_DIR,filenames[0]))\n",
    "feature_params=np.loadtxt(os.path.join(MODEL_DIR,filenames[1]))\n",
    "transition_gradient=np.loadtxt(os.path.join(MODEL_DIR,filenames[2]))\n",
    "transition_params=np.loadtxt(os.path.join(MODEL_DIR,filenames[3]))\n",
    " \n",
    "letterdict= {'e': 0, 't': 1, 'a': 2, 'i': 3, 'n': 4, 'o': 5, 's': 6, 'h': 7, 'r': 8, 'd': 9}    \n",
    "trainfiles=os.listdir(TRAIN_DIR)\n",
    "with open(os.path.join(TRAIN_DIR,'train_words.txt'),'r') as traindoc:\n",
    "    train_words=traindoc.read().split()\n",
    "\n",
    "\n",
    "testfiles=os.listdir(TEST_DIR)\n",
    "with open(os.path.join(TEST_DIR,'test_words.txt'),'r') as testdoc:\n",
    "    test_words=testdoc.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = list('etainoshrd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10L, 321L)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WF_cf\n",
    "feature_params.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10L, 10L)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WT_cc'\n",
    "transition_params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.371026,  0.120769, -0.194188, -0.385523,  0.107982, -0.197563,\n",
       "         0.414634,  0.272522,  0.288212,  0.278577],\n",
       "       [ 0.120769, -0.653739,  0.28069 ,  0.054133, -0.28821 ,  0.208675,\n",
       "        -0.377716,  0.365483, -0.105408, -0.252407],\n",
       "       [-0.194188,  0.28069 , -0.23071 ,  0.219636,  0.116347, -0.167909,\n",
       "        -0.248713,  0.047337,  0.200981,  0.066791],\n",
       "       [-0.385523,  0.054133,  0.219636, -0.328905,  0.306496, -0.026209,\n",
       "         0.286039, -0.07772 ,  0.22133 ,  0.01934 ],\n",
       "       [ 0.107982, -0.28821 ,  0.116347,  0.306496, -0.032171,  0.125915,\n",
       "         0.009618, -0.117396, -0.089888,  0.098682],\n",
       "       [-0.197563,  0.208675, -0.167909, -0.026209,  0.125915,  0.198127,\n",
       "        -0.134644,  0.008058,  0.220512,  0.028377],\n",
       "       [ 0.414634, -0.377716, -0.248713,  0.286039,  0.009618, -0.134644,\n",
       "        -0.320125, -0.009717, -0.322828, -0.005847],\n",
       "       [ 0.272522,  0.365483,  0.047337, -0.07772 , -0.117396,  0.008058,\n",
       "        -0.009717, -0.136171, -0.09005 , -0.118451],\n",
       "       [ 0.288212, -0.105408,  0.200981,  0.22133 , -0.089888,  0.220512,\n",
       "        -0.322828, -0.09005 ,  0.051519, -0.179775],\n",
       "       [ 0.278577, -0.252407,  0.066791,  0.01934 ,  0.098682,  0.028377,\n",
       "        -0.005847, -0.118451, -0.179775, -0.130758]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10L, 321L)"
      ]
     },
     "execution_count": 559,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10L, 10L)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_gradient.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train=[]\n",
    "X_test=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_words)):\n",
    "    X_train.append(np.loadtxt(os.path.join(TRAIN_DIR,\"train_img\"+str(i+1)+\".txt\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5L, 321L)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(test_words)):\n",
    "    X_test.append(np.loadtxt(os.path.join(TEST_DIR,\"test_img\"+str(i+1)+\".txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4L, 321L)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "letterdict['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train=[]\n",
    "Y_test=[]\n",
    "\n",
    "for i in range(len(train_words)):\n",
    "    Y_train.append(list(train_words[i]))\n",
    "    \n",
    "for i in range(len(test_words)):\n",
    "    Y_test.append(list(test_words[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., ...,  1.,  0.,  1.],\n",
       "       [ 1.,  1.,  1., ...,  0.,  1.,  1.],\n",
       "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 1.,  1.,  0., ...,  1.,  1.,  0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'h', 'a', 't']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def node_potentials(word, label_sequence ,feature_params):\n",
    "# feature_params,  k x n numpy array of feature parameter.\n",
    "# word ,a w x 1 numpy array of feature vectors,\n",
    "    calc= np.dot(word, np.transpose(feature_params))\n",
    "    potentials=np.zeros((len(label_sequence),1))\n",
    "    for i in range(len(label_sequence)):\n",
    "        potentials[i]= calc[i][alphabet.index(label_sequence[i])]\n",
    "    return np.exp(potentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.04885517e+08],\n",
       "       [  2.20913251e+01],\n",
       "       [  2.92388248e+07],\n",
       "       [  4.50596498e+10]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_that=node_potentials(X_test[0],Y_test[0],feature_params)\n",
    "node_that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 18.46838 ],\n",
       "       [  3.095185],\n",
       "       [ 17.191008],\n",
       "       [ 24.531253]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(node_that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "node_norm= normalize(node_that, norm='l1',axis=1)\n",
    "sum(node_norm[3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_partition(X_test, Y_test,feature_params,transition_params,n):\n",
    "# Exhaustive sum of the all label sequences for a word\n",
    "    # First three words\n",
    "    exhaustive_sum=[]\n",
    "\n",
    "    for k in range(n):\n",
    "        permutations= list(itertools.product(alphabet,repeat =len(Y_test[k])))\n",
    "        feature_sum=0\n",
    "        transition_sum=0\n",
    "        e_sum=0\n",
    "        for j in range(len(permutations)): \n",
    "            feat=node_potentials(X_test[k],permutations[j],feature_params)\n",
    "            feature_sum= reduce(lambda x,y: x*y,feat) \n",
    "            tran=(np.exp([transition_params[alphabet.index(permutations[j][i])][alphabet.index(permutations[j][i+1])] for i in range(len(permutations[j])-1)]))\n",
    "            transition_sum=reduce(lambda x,y: x*y,tran)\n",
    "            \n",
    "            e_sum+=(feature_sum*transition_sum)\n",
    "        exhaustive_sum.append(e_sum) \n",
    "    return (exhaustive_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exhaustive sum for three words is : \n",
      "[array([  2.28623899e+29]), array([  8.29939019e+38]), array([  9.15068082e+44])]\n",
      "In log scale \n",
      "[[  67.6018758 ]\n",
      " [  89.61441558]\n",
      " [ 103.52757238]]\n"
     ]
    }
   ],
   "source": [
    "es=log_partition(X_test, Y_test,feature_params,transition_params,3)\n",
    "print \"Exhaustive sum for three words is : \\n\",es\n",
    "print \"In log scale \\n\",np.log(es)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def energy (x, y,feature_params,transition_params):\n",
    "# Compute energy defined in question 1.2\n",
    "# inputs: input x, label sequence y, params\n",
    "# output: energy (a number)\n",
    "    node_pot=np.log(node_potentials(x,y,feature_params))\n",
    "    feature_sum= np.sum(node_pot)\n",
    "    transition_sum=sum([transition_params[alphabet.index(y[i])][alphabet.index(y[i+1])] for i in range(len(y)-1)])\n",
    "    return (feature_sum+transition_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Negative energy of the true label sequence,  ['t', 'h', 'a', 't'] is : 63.979336\n",
      " Negative energy of the true label sequence,  ['h', 'i', 'r', 'e'] is : 89.61093\n",
      " Negative energy of the true label sequence,  ['r', 'i', 's', 'e', 's'] is : 96.940634\n"
     ]
    }
   ],
   "source": [
    "E1=energy(X_test[0],Y_test[0],feature_params,transition_params)\n",
    "E2=energy(X_test[1],Y_test[1],feature_params,transition_params)\n",
    "E3=energy(X_test[2],Y_test[2],feature_params,transition_params)\n",
    "\n",
    "print \" Negative energy of the true label sequence, \",Y_test[0],\"is :\",E1\n",
    "print \" Negative energy of the true label sequence, \",Y_test[1],\"is :\",E2\n",
    "print \" Negative energy of the true label sequence, \",Y_test[2],\"is :\",E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_potentials(word,feature_params):\n",
    "# feature_params, C x n numpy array of feature parameter.\n",
    "# word ,a w x n numpy array of feature vectors,\n",
    "# word= X_test[i]\n",
    "    return np.dot(word, np.transpose(feature_params))\n",
    "    \n",
    "\n",
    "def transition_potential (feat_pot1,feat_pot2, transition_params):\n",
    "# Absorb node potential into a pairwise potential,\n",
    "# for positions (t, t+1).\n",
    "# output: (log) pairwise potential function.  (a table, e.g. array)\n",
    "# Returns a C x C numpy array, where k is the size of the alphabet.\n",
    "    tran_pot = transition_params+ feat_pot1[:, np.newaxis]\n",
    "    if feat_pot2 is not None:\n",
    "        tran_pot += feat_pot2\n",
    "    return tran_pot\n",
    "\n",
    "def chain_potentials(word,feature_params,transition_params):\n",
    "#Computes the clique potentials of the entire chain.\n",
    "#Returns a (w-1) x C x C numpy array,\n",
    "    phi = feature_potentials(word, feature_params)\n",
    "    \n",
    "    # Include the potentials of the last two nodes in the same clique\n",
    "    transitions = [(node, None) for node in phi[:-2]] + [(phi[-2], phi[-1])]\n",
    "    #transitions = [(node, None) for node in phi[:]]\n",
    "    psi = [transition_potential(node1, node2, transition_params) for node1, node2 in transitions]\n",
    "    return np.array(psi)\n",
    "\n",
    "\n",
    "def message_passing (psi):\n",
    "# Message passing algorithm\n",
    "# input: (log-) potential\n",
    "# outputs: forward/backward messages\n",
    "    # Backward messages\n",
    "    back = []\n",
    "    prev_msgs = np.zeros(psi.shape[1])\n",
    "    for pairs in psi[:0:-1]:\n",
    "        message = scipy.misc.logsumexp(pairs + prev_msgs, axis=1)\n",
    "        back.append(message)\n",
    "        prev_msgs += message\n",
    "\n",
    "    # Forward messages\n",
    "    fwd = []\n",
    "    prev_msgs = np.zeros(psi.shape[1])\n",
    "    for pairs in psi[:-1]:\n",
    "        message = scipy.misc.logsumexp(pairs + prev_msgs[:, np.newaxis], axis=0)\n",
    "        fwd.append(message)\n",
    "        prev_msgs += message\n",
    "\n",
    "    return (np.array(back), np.array(fwd))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward message array : \n",
      "[[ 18.58934459  17.81529477  18.74937334  18.52273392  18.18075315\n",
      "   18.67731044  18.0912883   18.83407002  18.3634186   18.21639565]\n",
      " [ 25.65107866  25.23685908  25.59838345  25.57794329  25.27163715\n",
      "   25.60124492  25.07146009  25.38802664  25.4145125   25.20264414]]\n",
      "Backward message array : \n",
      "[[ 14.4438841   24.77485617  42.00295108  12.5677121   29.82240312\n",
      "   24.14588509   2.72717112  34.04564006  33.90826111  26.22602211]\n",
      " [ 37.73533939  48.02906741  42.94954937  40.43003538  40.90760095\n",
      "   40.05103627  33.45509765  45.14603484  49.0110484   42.41188976]]\n",
      "\n",
      " \n",
      "\n",
      " Log-space message m1→2(Y2) :  18.834070015\n",
      " Log-space message m2→1(Y1) :  24.7748561679\n",
      " Log-space message m2→3(Y3) :  25.5983834534\n",
      " Log-space message m3→2(Y2) :  45.1460348364\n"
     ]
    }
   ],
   "source": [
    "psi_that=chain_potentials(X_test[0],feature_params,transition_params)\n",
    "back,forward=message_passing(psi_that)\n",
    "print \"Forward message array : \\n\", (forward)\n",
    "print \"Backward message array : \\n\", (back)\n",
    "print \"\\n \\n\"\n",
    "print \" Log-space message m1→2(Y2) : \", (forward[0][alphabet.index(Y_test[0][1])])\n",
    "print \" Log-space message m2→1(Y1) : \", (back[0][alphabet.index(Y_test[0][0])])\n",
    "print \" Log-space message m2→3(Y3) : \", (forward[1][alphabet.index(Y_test[0][2])])\n",
    "print \" Log-space message m3→2(Y2) : \", (back[1][alphabet.index(Y_test[0][1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def beliefs(word,feature_params,transition_params):\n",
    "# Returns a numpy array of size (w-1) x k x k,\n",
    "    psi = chain_potentials(word,feature_params,transition_params)\n",
    "    delta_bwd, delta_fwd = message_passing(psi)\n",
    "\n",
    "    k = delta_fwd.shape[1]\n",
    "    delta_fwd = np.concatenate(([np.zeros(k)], delta_fwd))\n",
    "    delta_bwd = np.concatenate((delta_bwd[::-1], [np.zeros(k)]))\n",
    "    belief = psi + delta_fwd[:, :, np.newaxis] + delta_bwd[:, np.newaxis]\n",
    "\n",
    "    return np.array(belief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pairwise_prob(belief):\n",
    "# pairwise marginal probabilities.\n",
    "    return np.exp(belief - scipy.misc.logsumexp(belief, axis=(1,2))[:, np.newaxis, np.newaxis])\n",
    "\n",
    "def single_prob(pairwise_p):\n",
    "# singleton marginal probabilities.\n",
    "\n",
    "    a = np.sum(pairwise_p, axis=2)\n",
    "    b = np.sum(pairwise_p[-1], axis=0) # Last character in the word\n",
    "\n",
    "    return np.concatenate((a, b[np.newaxis, :]))\n",
    "\n",
    "def joint_prob(single_p, label_sequence, alphabet):\n",
    "# Computes the joint probability of the label given singleton marginal probabilities.\n",
    "    prob = [np.log(marginal[alphabet.index(c)]) for (c, marginal) in zip(label_sequence, single_p)]\n",
    "\n",
    "    return np.exp(np.sum(prob))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3L, 10L, 10L)"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "belief_that= beliefs(X_test[0],feature_params,transition_params)\n",
    "pw_prob= pairwise_prob(belief_that)\n",
    "s_prob=single_prob(pw_prob)\n",
    "pw_prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.72360443e-01,   2.73053851e-03,   2.67297536e-02],\n",
       "       [  7.46583831e-12,   2.78595262e-14,   3.30864004e-13],\n",
       "       [  1.59042715e-11,   7.20009247e-14,   5.38968123e-13]])"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw_prob[0][[1,2,7],:][:,[1,2,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.23141736e-09,   1.72371206e-01,   6.56861166e-05],\n",
       "       [  1.49969782e-10,   2.72880257e-03,   1.26156644e-06],\n",
       "       [  1.21044035e-09,   2.67202731e-02,   7.78624591e-06]])"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw_prob[1][[1,2,7],:][:,[1,2,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.29451163e-08,   2.07955205e-24,   1.05809723e-21],\n",
       "       [  9.99458796e-01,   2.13367796e-17,   1.31708499e-14],\n",
       "       [  2.83525311e-04,   7.34319323e-21,   2.85705826e-18]])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pw_prob[2][[1,2,7],:][:,[1,2,7]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood_true(Y_train,X_train,alphabet,feature_params,transition_params,n):\n",
    "# Objective function to minimize.\n",
    "# Returns the negative average log likelihood of theta given the data.    \n",
    "    p = []\n",
    "    for i in range(len(Y_train[:n])):\n",
    "        belief = beliefs(X_train[i],feature_params,transition_params)\n",
    "        pairwise_p = pairwise_prob(belief)\n",
    "        single_p = single_prob(pairwise_p)\n",
    "        p.append(joint_prob(single_p,Y_train[i] , alphabet))\n",
    "    print p\n",
    "    return np.sum(p)/len(Y_train[:n])\n",
    "\n",
    "\n",
    "def likelihood_predict(Y_train,X_train,alphabet,feature_params,transition_params,n):\n",
    "# Objective function to minimize.\n",
    "# Returns the negative average log likelihood of theta given the data.    \n",
    "    p = []\n",
    "    for i in range(len(Y_train[:n])):\n",
    "        belief = beliefs(X_train[i],feature_params,transition_params)\n",
    "        pairwise_p = pairwise_prob(belief)\n",
    "        single_p = single_prob(pairwise_p)\n",
    "        p.append(joint_prob(single_p, predict_word(single_p, alphabet), alphabet))\n",
    "    print p\n",
    "    return np.sum(p)/len(Y_train[:n])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.97558341038452212, 0.99050814094478423, 0.00014428552955861841, 0.91662951088527911, 0.0053845825352322489, 0.007200290545782956, 0.99986641183040836, 0.002346236423977373, 2.6476486787640712e-07, 0.82023082117478052, 0.99988316906039953, 0.98511477871051001, 0.99433901609018771, 8.6020530317901239e-06, 0.076872350577993326, 2.2497166501472388e-06, 0.010336764665831675, 0.99790197753269594, 0.0052825534264687264, 1.8029298336690324e-09, 0.45815171980553171, 1.3252514148760273e-09, 0.85112901832845433, 0.42832926543684463, 2.6571939438687889e-06, 0.0017449701514608579, 0.044700394613537899, 0.99991281455599279, 0.02685200858821158, 0.91901107835611895, 0.99945352815662114, 0.00034387713967458941, 0.00015279635546277601, 1.9920725149157764e-05, 0.9959997341665715, 0.014044941027658409, 0.032465560296249751, 0.0032558355816338313, 0.99997897743278374, 0.99714389519924396, 0.99999544122737394, 0.99988663980724113, 0.99631869122263228, 0.00024119451419604357, 8.156979416313076e-06, 8.4218168437160099e-07, 0.0016641705241010614, 0.75984018399355946, 0.95713894090279839, 0.58870540294982765]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.43728256154790235"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_true(Y_train,X_train,alphabet,feature_params,transition_params,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.79578548353809586, 0.99652049240843565, 0.93700663528992467]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9097708704121521"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_predict(Y_test,X_test,alphabet,feature_params,transition_params,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_word(single_p, alphabet):\n",
    "# Returns a list of predicted characters of a word.\n",
    "# Parameters:\n",
    "#    - single_p, a w x k numpy array of singleton marginal probabilities,\n",
    "#      where w is the word length and k is the size of the alphabet; and\n",
    "#    - alphabet, a list of all possible character labels.\n",
    "    indices = np.argmax(single_p, axis=1)\n",
    "    \n",
    "    return [alphabet[i] for i in indices]\n",
    "\n",
    "def predict(Y_train,X_train,alphabet,feature_params,transition_params):\n",
    "# Returns a list of predictions, where each prediction is a list of predicted character labels of a word.\n",
    "    predictions = []\n",
    "    for i in range(len(Y_train)):\n",
    "        belief = beliefs(X_train[i],feature_params,transition_params)\n",
    "        pairwise_p = pairwise_prob(belief)\n",
    "        single_p = single_prob(pairwise_p)\n",
    "        predictions.append(predict_word(single_p, alphabet))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sequence for word 1 is :  ['t', 'r', 'a', 't'] with probability :  0.795785483538\n",
      "Predicted sequence for word 2 is :  ['h', 'i', 'r', 'e'] with probability :  0.996520492408\n",
      "Predicted sequence for word 3 is :  ['r', 'i', 's', 'e', 'r'] with probability :  0.93700663529\n"
     ]
    }
   ],
   "source": [
    "belief_that= beliefs(X_test[0],feature_params,transition_params)\n",
    "pw_prob1= pairwise_prob(belief_that)\n",
    "s_prob1=single_prob(pw_prob1)\n",
    "\n",
    "belief_hire= beliefs(X_test[1],feature_params,transition_params)\n",
    "pw_prob2= pairwise_prob(belief_hire)\n",
    "s_prob2=single_prob(pw_prob2)\n",
    "\n",
    "belief_rises= beliefs(X_test[2],feature_params,transition_params)\n",
    "pw_prob3= pairwise_prob(belief_rises)\n",
    "s_prob3=single_prob(pw_prob3)\n",
    "print \"Predicted sequence for word 1 is : \",predict_word(s_prob1, alphabet),\"with probability : \",joint_prob(s_prob1, predict_word(s_prob1, alphabet), alphabet)\n",
    "print \"Predicted sequence for word 2 is : \",predict_word(s_prob2, alphabet),\"with probability : \",joint_prob(s_prob2, predict_word(s_prob2, alphabet), alphabet)\n",
    "print \"Predicted sequence for word 3 is : \",predict_word(s_prob3, alphabet),\"with probability : \",joint_prob(s_prob3, predict_word(s_prob3, alphabet), alphabet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'h', 'e', 'o', 'd', 'o', 'r', 'e', 's'],\n",
       " ['i', 'i', 't', 's', 'e'],\n",
       " ['n', 'i', 'a', 't', 'h', 't'],\n",
       " ['h', 'a', 'n', 's', 'e'],\n",
       " ['s', 'i', 'a', 'e', 's'],\n",
       " ['i', 'r', 't', 's', 'h', 'e'],\n",
       " ['e', 'r', 'i', 't', 'r', 'e', 'a', 't'],\n",
       " ['s', 'e', 'a', 'e', 'o', 'n', 'i'],\n",
       " ['s', 'n', 'e', 'e', 't', 'e'],\n",
       " ['s', 'a', 'n', 't', 'o', 's', 'e'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['h', 'a', 'r', 't', 'h'],\n",
       " ['n', 'o', 't', 'i', 'o', 'n', 'i'],\n",
       " ['a', 'r', 't', 'r', 'e', 's'],\n",
       " ['h', 'a', 'a', 't'],\n",
       " ['i', 'n', 'r', 't', 'e', 'a', 'd', 'e'],\n",
       " ['a', 'o', 's', 'a', 's', 's', 'i', 'd', 'a', 't', 'i', 'o', 'n', 'e'],\n",
       " ['t', 'e', 'r', 'r', 'a', 'i', 'n', 'i'],\n",
       " ['s', 't', 'd', 'n', 'e', 's', 'e'],\n",
       " ['e', 'a', 's', 't', 'e', 'r', 'e'],\n",
       " ['h', 'i', 'r', 'd', 'e'],\n",
       " ['i', 't', 'd', 'o', 't', 'e', 's', 'i', 'h', 'r', 'e'],\n",
       " ['r', 'e', 's', 'i', 'd', 'e', 's'],\n",
       " ['t', 'n', 'o', 's', 'e', 's'],\n",
       " ['t', 'n', 't', 'n', 'i', 'o', 'r'],\n",
       " ['s', 'r', 'i', 'n'],\n",
       " ['i', 'h', 'a', 't', 'h'],\n",
       " ['t', 'i', 'e', 's'],\n",
       " ['d', 't', 'h', 'n', 'a', 't'],\n",
       " ['h', 'i', 'd', 'd', 'e', 'n', 'i'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['a', 's', 't', 'e', 'r', 't', 'i', 'd', 'e'],\n",
       " ['r', 'n', 'd', 'e'],\n",
       " ['a', 'o', 't', 'h'],\n",
       " ['r', 'h', 'o', 'd', 'e', 's'],\n",
       " ['d', 'e', 'n', 's', 'i', 't', 'i', 'r', 's', 'e'],\n",
       " ['n', 'e', 'o', 'r'],\n",
       " ['d', 'i', 's', 's', 'e', 'n', 'n', 'i'],\n",
       " ['a', 'i', 'd', 'e', 'd', 'e'],\n",
       " ['e', 'a', 's', 'e', 's'],\n",
       " ['t', 'h', 'e', 'i', 'r', 'e'],\n",
       " ['t', 'h', 'a', 'i', 'n'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['s', 'a', 'r', 'n', 'h', 't'],\n",
       " ['h', 'e', 'a', 'h', 't'],\n",
       " ['d', 't', 'e', 'o', 'r'],\n",
       " ['r', 'e', 'n', 's', 'e', 's'],\n",
       " ['a', 'n', 'd', 'e'],\n",
       " ['a', 'r', 't', 'i', 's', 't', 's', 'e'],\n",
       " ['s', 'h', 'o', 'e', 's'],\n",
       " ['i', 'o', 'n', 's', 'e'],\n",
       " ['a', 't', 'r', 'e', 's', 't', 's', 'e'],\n",
       " ['h', 'e', 't', 's', 'e'],\n",
       " ['e', 'r', 'o', 'o', 'e', 'o', 't'],\n",
       " ['e', 'r', 'n', 'e', 's', 't', 'h'],\n",
       " ['r', 'e', 't', 'i', 'r', 'e', 'd', 'e'],\n",
       " ['h', 'i', 'd', 'e', 's'],\n",
       " ['i', 'o', 'n', 'i'],\n",
       " ['a', 't', 'n', 'e', 'e', 't', 'e'],\n",
       " ['d', 'a', 't', 'e', 's', 'e'],\n",
       " ['d', 'e', 's', 'e', 'r', 't', 'h'],\n",
       " ['a', 's', 'h', 'e', 'o', 'r'],\n",
       " ['n', 'o', 't', 'a', 't', 'i', 'o', 'n', 'i'],\n",
       " ['a', 'i', 'n', 'i'],\n",
       " ['d', 'o', 'a', 'i', 'n', 'i'],\n",
       " ['i', 'n', 't', 'e', 'r', 'i', 'o', 'r', 'e'],\n",
       " ['r', 's', 't', 'r', 'e', 'a', 't', 'e', 'i', 'n'],\n",
       " ['n', 'n', 'n', 'a', 't'],\n",
       " ['e', 'd', 'd', 'i', 'e', 's'],\n",
       " ['i', 'o', 'o', 't', 'e', 'd', 'e'],\n",
       " ['t', 'a', 's', 't', 'e', 's'],\n",
       " ['h', 'o', 's', 't', 's', 'e'],\n",
       " ['i', 'n', 'd', 'o', 'o', 'r', 'e'],\n",
       " ['s', 'h', 'o', 'o', 't', 'h'],\n",
       " ['h', 'e', 'r', 'o', 'e', 'r', 'e'],\n",
       " ['a', 't', 'h', 'e', 'i', 's', 't', 's', 'e'],\n",
       " ['e', 'h', 'a', 'h', 't'],\n",
       " ['s', 'a', 'n', 't', 'o', 'r'],\n",
       " ['h', 'i', 'r', 'e', 'd', 'e'],\n",
       " ['h', 'a', 'r', 'r', 'i', 'd', 'o', 'n', 'i'],\n",
       " ['s', 'i', 'r', 'e'],\n",
       " ['n', 'o', 'r', 'd', 'r', 's', 'e'],\n",
       " ['s', 'i', 'a', 'e', 's', 'e'],\n",
       " ['d', 'e', 's', 'e'],\n",
       " ['o', 'n', 'e', 's'],\n",
       " ['r', 'o', 'a', 't'],\n",
       " ['h', 't', 's', 'e'],\n",
       " ['i', 'n', 'e', 'e', 'n', 't', 't', 'o', 'n', 's', 'e'],\n",
       " ['s', 'h', 'o', 'r', 'e', 's', 'e'],\n",
       " ['t', 'r', 'e', 'a', 't', 'i', 'e', 's', 'e'],\n",
       " ['t', 'n', 'i', 'e', 'e', 'd', 'e'],\n",
       " ['t', 'o', 'o', 'n', 's', 'e'],\n",
       " ['a', 'o', 'i', 'n'],\n",
       " ['d', 'a', 't', 'e', 's'],\n",
       " ['a', 's', 's', 'a', 's', 's', 'i', 'n', 'h', 't', 'e', 'd', 'e'],\n",
       " ['i', 'n', 'd', 'o', 'n', 'e', 's', 'i', 'a', 't'],\n",
       " ['i', 'n', 't', 'e', 'r', 'e', 's', 't', 's', 'e'],\n",
       " ['n', 'e', 'e', 'i', 'r', 'e'],\n",
       " ['i', 'd', 'e', 'a', 't'],\n",
       " ['h', 'i', 's', 'e'],\n",
       " ['s', 'e', 'a', 't', 'e', 'd', 'e'],\n",
       " ['t', 'e', 'a', 'r', 'e'],\n",
       " ['r', 'a', 's', 'o', 'r'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'i'],\n",
       " ['t', 'h', 'n', 'n', 'i'],\n",
       " ['o', 't', 't', 'o', 'r'],\n",
       " ['d', 'o', 'o', 't', 's', 'e'],\n",
       " ['t', 'h', 'r', 'e', 'e', 's'],\n",
       " ['r', 'i', 'a', 't', 'a', 't'],\n",
       " ['e', 's', 't', 'o', 'n', 'i', 'a', 't'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['t', 'r', 'e', 'a', 'd', 'e'],\n",
       " ['a', 'i', 't', 'e', 'n', 'd', 'e', 'd', 'e'],\n",
       " ['s', 'h', 'o', 'r', 't', 's', 'e'],\n",
       " ['d', 'r', 'h', 'i', 'n', 'e', 'o', 'r'],\n",
       " ['o', 'a', 'n', 'i'],\n",
       " ['d', 'o', 'h', 'a', 't'],\n",
       " ['r', 'h', 'e', 's'],\n",
       " ['s', 's', 'n', 'a', 'i', 'n'],\n",
       " ['t', 'd', 'a', 'd', 'e'],\n",
       " ['d', 'a', 't', 'a', 't'],\n",
       " ['n', 'o', 't', 'e', 's', 'e'],\n",
       " ['e', 'a', 't', 'h'],\n",
       " ['d', 'e', 'n', 'n', 'i', 's', 'e'],\n",
       " ['r', 'a', 't', 'a', 't', 'i', 'o', 'h', 't'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['s', 'i', 't', 's', 'e'],\n",
       " ['t', 'h', 'a', 't', 'h'],\n",
       " ['d', 'r', 'e', 's', 'd', 'e', 'n', 'e'],\n",
       " ['t', 'h', 'e', 'a', 't', 'e', 'r', 's', 'e'],\n",
       " ['d', 'r', 'e', 't', 's', 'e', 'd', 'e'],\n",
       " ['s', 'o', 'r', 't', 's', 'e'],\n",
       " ['o', 'r', 'n', 'a', 't', 'e', 's'],\n",
       " ['s', 'a', 'h', 'a', 'r', 'a', 't'],\n",
       " ['a', 'i', 'h', 'e', 't', 'e', 'r', 'n', 'i'],\n",
       " ['a', 'd', 'd', 'e'],\n",
       " ['s', 't', 'a', 'r', 'e'],\n",
       " ['e', 'a', 'r', 't', 'a', 't'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['d', 'i', 's', 'o', 't', 'd', 'e', 'r', 'e'],\n",
       " ['a', 'h', 'e', 'd', 'e'],\n",
       " ['r', 'a', 'i', 'o', 's', 'e'],\n",
       " ['h', 'h', 'e', 'a', 'd', 'e'],\n",
       " ['a', 'n', 'd', 'e'],\n",
       " ['t', 'r', 'a', 'd', 'i', 't', 'i', 'o', 'n', 'i'],\n",
       " ['s', 'a', 'n', 'd', 's', 'i', 'o', 'n', 'e', 's'],\n",
       " ['t', 'e', 'n', 's', 'i', 't', 'h', 's', 'e'],\n",
       " ['d', 'o', 's', 'r', 's', 'e'],\n",
       " ['s', 't', 'h', 't', 'i', 'd', 'r', 's', 'e'],\n",
       " ['i', 'r', 'a', 'n', 'i', 'a', 'd', 'e'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['t', 'a', 'o', 'i', 's', 't', 'h'],\n",
       " ['s', 't', 'r', 'a', 'i', 'n', 's', 'e'],\n",
       " ['t', 'e', 'r', 'e', 's', 'a', 't'],\n",
       " ['t', 'h', 'e', 's', 'e'],\n",
       " ['t', 'e', 't', 'd', 'e', 'r', 'e', 'd', 'e'],\n",
       " ['i', 'e', 't', 't', 'i', 'r', 's', 'e'],\n",
       " ['d', 'e', 'a', 'd', 'e'],\n",
       " ['t', 'r', 'a', 'i', 'n', 'e', 'd', 'e'],\n",
       " ['h', 'a', 'a', 'd', 'e'],\n",
       " ['s', 'i', 's', 't', 'e', 'h', 's', 'e'],\n",
       " ['a', 'i', 'r', 'e', 't', 'h'],\n",
       " ['h', 'i', 's', 'e'],\n",
       " ['i', 'n', 't', 'e', 'r', 'r', 't', 'a', 't', 'e', 's'],\n",
       " ['n', 'e', 'i', 't', 'h', 'e', 'r', 'e'],\n",
       " ['d', 'o', 't', 'h'],\n",
       " ['s', 't', 'a', 't', 'e', 'h', 'o', 'o', 'd', 'e'],\n",
       " ['n', 'i', 's', 'e'],\n",
       " ['n', 'o', 'r', 't', 'n', 'e', 'r', 'n', 'i'],\n",
       " ['t', 'i', 'r', 'd', 'e'],\n",
       " ['r', 'e', 's', 'i', 'd', 'e', 'n', 't', 's', 'e'],\n",
       " ['t', 'o', 'd', 'o', 'o', 'd', 'e'],\n",
       " ['a', 'r', 'd', 'e'],\n",
       " ['d', 'e', 'e', 'r', 'e'],\n",
       " ['s', 'h', 'o', 'r', 't', 'e', 'r', 'e'],\n",
       " ['s', 't', 'o', 'r', 'e', 's', 'e'],\n",
       " ['i', 'a', 'd', 'i', 'a', 'n', 'i'],\n",
       " ['d', 'n', 'a', 't'],\n",
       " ['e', 'n', 'd', 'e', 'd', 'e'],\n",
       " ['n', 'e', 's', 'e'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['d', 'i', 's', 'a', 's', 't', 'e', 'r', 's', 'e'],\n",
       " ['a', 'a', 'd', 'e'],\n",
       " ['t', 'h', 'd', 'i', 'a', 'a', 'a', 't'],\n",
       " ['d', 'e', 'r', 't', 'i', 'n', 'e', 't', 'i', 'o', 'n', 'i'],\n",
       " ['r', 'e', 'a', 't', 's', 'e'],\n",
       " ['t', 'e', 'n', 'n', 'i', 's', 'e'],\n",
       " ['h', 'e', 'r', 'i', 'n', 'i'],\n",
       " ['r', 'a', 't', 'e', 's'],\n",
       " ['t', 'n', 'e', 'r', 'e', 's'],\n",
       " ['o', 'e', 'a', 't', 'n', 's', 'e'],\n",
       " ['e', 'a', 't', 't', 'o', 'r', 'e'],\n",
       " ['r', 'e', 'e', 'o', 'r'],\n",
       " ['d', 'r', 'a', 'i', 'n', 's', 'e'],\n",
       " ['s', 'e', 'a', 's', 'o', 'n', 'd', 'e'],\n",
       " ['t', 'r', 'r', 'e', 's'],\n",
       " ['s', 'a', 'n', 't', 'a', 't'],\n",
       " ['a', 'd', 'd', 'r', 'e', 's', 's', 'e'],\n",
       " ['r', 'r', 'i', 'e', 's'],\n",
       " ['a', 'i', 'd', 's', 'e'],\n",
       " ['d', 't', 'e', 's', 'e'],\n",
       " ['r', 'e', 's', 'o', 'r', 't', 's', 'e'],\n",
       " ['s', 't', 'r', 'a', 'i', 'n', 'i'],\n",
       " ['s', 'i', 'o', 'r', 'i', 'e', 't', 'h'],\n",
       " ['a', 'n', 'd', 'e'],\n",
       " ['a', 's', 'i', 'a', 't'],\n",
       " ['s', 'e', 't', 'h'],\n",
       " ['a', 'e', 'i', 'r', 'e'],\n",
       " ['e', 'r', 'o', 's', 'i', 'o', 'n', 'i'],\n",
       " ['s', 'e', 'e', 's', 'e'],\n",
       " ['d', 'a', 't', 's', 'o', 'n', 'i'],\n",
       " ['h', 'o', 'r', 'r', 's', 'e'],\n",
       " ['r', 'e', 'a', 'd', 's', 'e'],\n",
       " ['i', 'n', 's', 'i', 'd', 'e', 's'],\n",
       " ['a', 'r', 't', 'i', 's', 't', 'h'],\n",
       " ['s', 'a', 'n', 'i'],\n",
       " ['e', 's', 't', 'a', 't', 'e', 's', 'e'],\n",
       " ['a', 'o', 'n', 'e', 's'],\n",
       " ['o', 'n', 'e', 's'],\n",
       " ['e', 'a', 's', 't', 'e'],\n",
       " ['e', 'a', 't', 'e', 't', 's', 'e'],\n",
       " ['s', 'i', 't', 'e', 's', 'e'],\n",
       " ['d', 'i', 'e', 's'],\n",
       " ['t', 'n', 'a', 'd', 'e', 'e'],\n",
       " ['t', 'r', 'i', 'e', 'd', 'e'],\n",
       " ['t', 'r', 'i', 'o', 'r'],\n",
       " ['e', 'i', 't', 'h'],\n",
       " ['t', 'i', 'n', 'i'],\n",
       " ['r', 'h', 'o', 'd', 'e', 's', 'i', 'a', 't'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['a', 'r', 'r', 'e', 's', 't', 'e', 'i', 'n'],\n",
       " ['d', 'e', 's', 't', 'i', 'n', 'a', 't', 'i', 'o', 'n', 's', 'e'],\n",
       " ['t', 'a', 'e', 'r', 'e'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['o', 'r', 'i', 'e', 'n', 't', 'e', 'd', 'e'],\n",
       " ['r', 'i', 't', 'e', 's', 'e'],\n",
       " ['t', 'a', 's', 't', 'e', 's', 'e'],\n",
       " ['s', 'e', 'a', 's', 'e', 's', 'e'],\n",
       " ['t', 'r', 'e', 'a', 't', 'i', 's', 'e', 's'],\n",
       " ['s', 't', 'a', 'n', 'd', 'e'],\n",
       " ['i', 'n', 's', 'e', 'r', 't', 'e', 'a', 't'],\n",
       " ['i', 'n', 't', 'e', 'r', 'r', 'e', 'o', 'e'],\n",
       " ['t', 'n', 't', 'r', 'd', 's', 'e'],\n",
       " ['t', 'r', 'e', 'a', 'o', 'o', 'n', 'i'],\n",
       " ['r', 'a', 'r', 'e', 's'],\n",
       " ['i', 'd', 's', 'e', 'a', 's', 'e', 'r', 'e'],\n",
       " ['t', 'h', 'e', 's'],\n",
       " ['r', 'i', 't', 'e', 's'],\n",
       " ['n', 'o', 'n', 'e', 's', 'e'],\n",
       " ['a', 'n', 'd', 'e'],\n",
       " ['r', 'e', 'a', 'd', 'e', 'r', 's', 'e'],\n",
       " ['h', 't', 'd', 'e'],\n",
       " ['d', 'e', 'r', 'e'],\n",
       " ['s', 't', 'a', 'r', 't', 'h'],\n",
       " ['r', 'a', 'i', 's', 'e', 's'],\n",
       " ['e', 'a', 't', 'e', 'n', 'i'],\n",
       " ['s', 'i', 't', 'e', 's'],\n",
       " ['r', 'e', 'n', 'd', 'e', 'r', 'e'],\n",
       " ['t', 'e', 's', 't', 'h'],\n",
       " ['h', 'a', 'a', 's', 'e', 'd', 'e'],\n",
       " ['h', 'e', 'a', 'n', 'i'],\n",
       " ['h', 'a', 'r', 'r', 'i', 's', 'e'],\n",
       " ['h', 'i', 'n', 'd', 'i', 'n'],\n",
       " ['n', 'e', 'e', 'd', 'e', 'i', 'n'],\n",
       " ['t', 'e', 'a', 'i', 's', 'e'],\n",
       " ['t', 'e', 'n', 'r', 'h', 'n', 'i'],\n",
       " ['r', 'e', 'd', 'e'],\n",
       " ['t', 'h', 'a', 't', 'h'],\n",
       " ['e', 'd', 'i', 't', 'i', 'o', 'h', 't'],\n",
       " ['s', 't', 'o', 'r', 'e', 's'],\n",
       " ['t', 'o', 'r', 'n', 'a', 'd', 'o', 'e', 's', 'e'],\n",
       " ['h', 'a', 't', 'h'],\n",
       " ['t', 'o', 'n', 'n', 'e', 's', 'e'],\n",
       " ['i', 'd', 'e', 'a', 's', 'e'],\n",
       " ['i', 'n', 'd', 'i', 'a', 'n', 'a', 't'],\n",
       " ['r', 'e', 'n', 's', 'o', 'h', 't'],\n",
       " ['d', 'r', 'i', 'e', 'r', 'e'],\n",
       " ['i', 's', 'a', 'r', 'e'],\n",
       " ['s', 'a', 'i', 'd', 'e'],\n",
       " ['h', 'n', 'd', 'e'],\n",
       " ['s', 'e', 't', 's', 'e'],\n",
       " ['t', 'e', 'r', 'r', 'o', 'r', 't', 's', 't', 'h'],\n",
       " ['t', 'n', 'a', 't', 'h'],\n",
       " ['a', 't', 't', 'n', 'i', 'n', 'i'],\n",
       " ['i', 't', 's', 'e'],\n",
       " ['s', 'e', 'a', 'h', 't', 'o', 'r', 'e'],\n",
       " ['t', 'h', 'r', 'r', 'a', 't', 'e', 'a', 't'],\n",
       " ['t', 'h', 'e', 'o', 'r', 'i', 's', 't', 's', 'e'],\n",
       " ['r', 'i', 'o', 't', 'h'],\n",
       " ['d', 'e', 'n', 'o', 't', 'e', 'd', 'e'],\n",
       " ['t', 'h', 'a', 't', 'a'],\n",
       " ['t', 'i', 'd', 'e', 's', 'e'],\n",
       " ['s', 'e', 'r', 'i', 'e', 'i', 'n'],\n",
       " ['s', 'h', 'o', 'r', 't', 'h'],\n",
       " ['t', 'n', 'e', 'a', 'i', 's', 'e'],\n",
       " ['e', 'r', 'r', 'o', 'n', 'i'],\n",
       " ['n', 'e', 'a', 'r', 'e', 's', 't', 'a'],\n",
       " ['h', 's', 't', 'h'],\n",
       " ['a', 't', 't', 'h', 'i', 'h', 'e', 'd', 'e'],\n",
       " ['d', 'e', 't', 'r', 'o', 'i', 't', 'h'],\n",
       " ['o', 'h', 'i', 'o', 'r'],\n",
       " ['d', 'e', 'n', 'o', 't', 'e', 's'],\n",
       " ['i', 't', 's', 'e'],\n",
       " ['a', 'r', 'e', 't'],\n",
       " ['t', 'n', 'e', 's'],\n",
       " ['h', 'o', 'r', 'r', 'o', 'o', 'r'],\n",
       " ['e', 'r', 'r', 't', 'r', 's', 'e'],\n",
       " ['t', 'a', 'e', 's'],\n",
       " ['a', 's', 'i', 'r', 'i', 'a', 't'],\n",
       " ['t', 'h', 'e', 's', 'e', 's'],\n",
       " ['t', 'r', 'a', 'a', 'i', 't', 'i', 'o', 'h', 's', 'e'],\n",
       " ['a', 'n', 'o', 't', 'h', 'e', 'r', 'e'],\n",
       " ['r', 'e', 's', 'i', 'r', 't', 'h'],\n",
       " ['t', 'a', 't', 'e', 's'],\n",
       " ['t', 'r', 'e', 'e', 's', 'e'],\n",
       " ['n', 'a', 'r', 'd', 'e'],\n",
       " ['a', 't', 'h', 'e', 'n', 's', 'e'],\n",
       " ['h', 'i', 's', 't', 'o', 'r', 'i', 'e', 'e', 's'],\n",
       " ['n', 'i', 'n', 'e', 's'],\n",
       " ['s', 'e', 'n', 'i', 'o', 'r', 'e'],\n",
       " ['e', 's', 't', 'o', 'n', 'i', 'a', 'h', 't'],\n",
       " ['t', 'e', 'e', 't', 'h', 'e'],\n",
       " ['h', 'o', 't', 'o', 'r', 'e', 'o', 'r'],\n",
       " ['i', 'n', 'h', 'e', 'r', 'i', 't', 'e', 'd', 'e'],\n",
       " ['t', 'e', 'r', 'r', 'i', 't', 'o', 'r', 'i', 'e', 's', 'e'],\n",
       " ['h', 'o', 'r', 'i', 'o', 'n', 's', 'e'],\n",
       " ['a', 'r', 'e', 's'],\n",
       " ['a', 's', 'h', 't'],\n",
       " ['h', 'a', 's', 'e'],\n",
       " ['e', 'a', 's', 't', 'e', 'n', 'n', 'i'],\n",
       " ['s', 'i', 's', 't', 'e', 'r', 'e'],\n",
       " ['a', 'i', 'd', 'r', 'e', 's', 's', 'e', 's', 'e'],\n",
       " ['r', 'o', 'a', 'd', 's', 'e'],\n",
       " ['t', 'h', 'i', 'r', 't', 'e', 'e', 'n', 'i'],\n",
       " ['s', 't', 'a', 'n', 'd', 'a', 'r', 'd', 's', 'e'],\n",
       " ['s', 't', 'a', 'r', 'r', 'e', 'd', 'e'],\n",
       " ['t', 'h', 'r', 'd', 'a', 'o', 'r'],\n",
       " ['a', 'r', 'e', 's'],\n",
       " ['n', 'a', 'i', 's', 'e', 'd', 'e'],\n",
       " ['s', 'a', 'r', 'd', 't', 'h'],\n",
       " ['s', 'i', 'o', 'r', 'r', 'a', 't'],\n",
       " ['s', 's', 'r', 'e'],\n",
       " ['r', 'r', 'o', 's', 's', 'e'],\n",
       " ['r', 'e', 's', 'i', 'd', 'e', 'a', 't'],\n",
       " ['a', 'n', 'd', 'e'],\n",
       " ['a', 's', 's', 'e', 'n', 't', 'h'],\n",
       " ['a', 's', 's', 'e', 'r', 't', 's', 'e'],\n",
       " ['t', 'e', 'a', 't'],\n",
       " ['s', 'e', 'n', 's', 'e', 's'],\n",
       " ['e', 'n', 't', 'e', 'r', 'e', 's', 'e'],\n",
       " ['e', 'a', 't', 'o', 'r'],\n",
       " ['e', 'a', 'r', 'n', 'e', 'd', 'e'],\n",
       " ['r', 't', 'd', 'e', 's'],\n",
       " ['a', 'i', 'r', 'e', 's', 'e'],\n",
       " ['e', 'n', 'r', 'i', 't', 'i', 'e', 's', 'e'],\n",
       " ['r', 'e', 's', 'o', 'r', 'i', 't'],\n",
       " ['t', 'r', 'e', 'a', 't', 'i', 's', 'e', 's', 'e'],\n",
       " ['s', 'r', 'a', 't', 'e', 'd', 'e'],\n",
       " ['t', 'r', 'e', 'r', 'd', 's', 'e'],\n",
       " ['i', 'd', 'e', 'n', 't', 'i', 't', 'i', 'e', 'd', 'e'],\n",
       " ['t', 'a', 'e', 's'],\n",
       " ['a', 't', 'h', 'e', 'i', 's', 't', 'h'],\n",
       " ['a', 's', 's', 'i', 's', 't', 'a', 'n', 't', 'h'],\n",
       " ['h', 'a', 'i', 't', 'i', 'a', 'n', 'i'],\n",
       " ['t', 'n', 'r', 'e', 'a', 't', 's', 'e'],\n",
       " ['t', 'e', 'd', 'e'],\n",
       " ['h', 'e', 'a', 'r', 'e', 'd', 'e'],\n",
       " ['i', 'n', 't', 'e', 'r', 'e', 's', 't', 'e'],\n",
       " ['o', 'r', 'i', 'e', 'n', 't', 'a', 't', 'i', 'o', 'n', 'i'],\n",
       " ['r', 'e', 's', 't', 'h'],\n",
       " ['t', 'r', 'e', 'a', 't', 'h'],\n",
       " ['h', 'e', 'a', 'd', 'e'],\n",
       " ['a', 'n', 'i', 'n'],\n",
       " ['s', 'o', 'n', 'a', 't', 'n', 'i'],\n",
       " ['r', 'e', 'a', 't', 'h'],\n",
       " ['s', 'e', 'e', 's'],\n",
       " ['h', 'a', 'i', 'n'],\n",
       " ['h', 'a', 's', 'e'],\n",
       " ['h', 'a', 'n', 'i'],\n",
       " ['a', 's', 's', 'i', 's', 't', 'h'],\n",
       " ['a', 'r', 'r', 'r', 's', 't', 'h'],\n",
       " ['r', 'e', 's', 't', 'o', 'r', 'e', 's'],\n",
       " ['s', 't', 'r', 'a', 'n', 'd', 'e'],\n",
       " ['t', 'i', 'e', 's', 'e'],\n",
       " ['s', 'h', 'e', 's'],\n",
       " ['a', 's', 's', 'e', 's', 's', 'e', 'd', 'e'],\n",
       " ['s', 'e', 'e', 'd', 'e', 'd', 'e'],\n",
       " ['a', 's', 'i', 'd', 'a', 't'],\n",
       " ['t', 'h', 'i', 's', 'e'],\n",
       " ['t', 'h', 'r', 'e'],\n",
       " ['e', 'n', 'd', 'o', 'r', 's', 'e', 'i', 'a'],\n",
       " ['s', 'e', 'a', 'd', 's', 'e'],\n",
       " ['t', 'r', 'a', 'n', 'e', 'i', 't', 'i', 'o', 'n', 'i'],\n",
       " ['h', 'a', 's', 'e'],\n",
       " ['i', 'n', 't', 'e', 'n', 'a', 'e', 'o', 'r'],\n",
       " ['n', 'o', 'n', 'i'],\n",
       " ['t', 'o', 'd', 'd', 'e'],\n",
       " ['t', 'e', 'n', 'd', 'r', 'e'],\n",
       " ['r', 'a', 'i', 'i', 'n'],\n",
       " ['i', 's', 'i', 's', 'e']]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(Y_train,X_train,alphabet,feature_params,transition_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'r', 'a', 't'],\n",
       " ['h', 'i', 'r', 'e'],\n",
       " ['r', 'i', 's', 'e', 'r'],\n",
       " ['e', 'd', 'i', 's', 'o', 'n'],\n",
       " ['s', 'h', 'o', 'r', 'e']]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(Y_test,X_test,alphabet,feature_params,transition_params)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['t', 'h', 'a', 't'],\n",
       " ['h', 'i', 'r', 'e'],\n",
       " ['r', 'i', 's', 'e', 's'],\n",
       " ['e', 'd', 'i', 's', 'o', 'n'],\n",
       " ['s', 'h', 'o', 'r', 'e']]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prediction_accuracy(Y_test,X_test,alphabet,feature_params,transition_params,n):\n",
    "# Returns accuracy of the sequence predcition task\n",
    "# Limiting to n words\n",
    "    predictions= predict(Y_test,X_test,alphabet,feature_params,transition_params)\n",
    "    total=0\n",
    "    correct=0\n",
    "    for i in range(n):\n",
    "        total+=len(Y_test[i])\n",
    "        count=0\n",
    "        for j in range(len(Y_test[i])):\n",
    "            if predictions[i][j]==Y_test[i][j]:\n",
    "                count+=1\n",
    "        correct+= count\n",
    "    return 100*correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.66666666666667"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_accuracy(Y_test,X_test,alphabet,feature_params,transition_params,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_grad(Y_train,X_train,alphabet,feature_params):\n",
    "# Returns a flattened k x n numpy array,where k is the size of the alphabet and n is the length of the feature vector.\n",
    "\n",
    "\n",
    "    # Initialize a state gradient table of size k x n with zeros\n",
    "    gradient = np.zeros((len(alphabet), len(X_train[0][1])))\n",
    "\n",
    "    for i in range(len(Y_train[:50])):\n",
    "        belief = beliefs(X_train[i],feature_params,transition_params)\n",
    "        pairwise_p = pairwise_prob(belief)\n",
    "        single_p = single_prob(pairwise_p)\n",
    "        for v, c, p in zip(X_train[i], Y_train[i], single_p):\n",
    "            for i in range(gradient.shape[0]): # possible labels\n",
    "                for j in range(gradient.shape[1]): # features\n",
    "                    indicator = 0\n",
    "                    if c == alphabet[i]:\n",
    "                        indicator = 1\n",
    "                    gradient[i][j] += (indicator - p[i]) * v[j]\n",
    "    \n",
    "    gradient /= len(Y_train[:50])\n",
    "\n",
    "    return np.ndarray.flatten((gradient))\n",
    "\n",
    "def transition_grad(Y_train,X_train,alphabet,feature_params,transition_params):\n",
    "# Returns a flattened k x k numpy array, where k is the size of the alphabet.\n",
    "\n",
    "    # Initialize a transition gradient table of size k x k with zeros\n",
    "    gradient = np.zeros((len(alphabet), len(alphabet)))\n",
    "\n",
    "    for i in range(len(Y_train[:50])):\n",
    "        belief = beliefs(X_train[i],feature_params,transition_params)\n",
    "        pairwise_p = pairwise_prob(belief)\n",
    "        label_pairs = zip([None] + Y_train[i], Y_train[i] + [None])[1:-1]\n",
    "\n",
    "        for (label1, label2), p in zip(label_pairs, pairwise_p):\n",
    "            for i in range(gradient.shape[0]):\n",
    "                for j in range(gradient.shape[1]):\n",
    "                    indicator = 0\n",
    "                    if label1 == alphabet[i] and label2 == alphabet[j]:\n",
    "                        indicator = 1\n",
    "                    gradient[i][j] += indicator - p[i][j]\n",
    "\n",
    "    gradient /= len(Y_train[:50])\n",
    "\n",
    "    return np.ndarray.flatten((gradient))\n",
    "\n",
    "def likelihood_prime(Y_train,X_train,alphabet,feature_params,transition_params):\n",
    "# Returns a flattened numpy array of the [feature_gradient, transition_gradient] list.\n",
    "\n",
    "    return np.concatenate((feature_grad(Y_train,X_train,alphabet,feature_params),\n",
    "                           transition_grad(Y_train,X_train,alphabet,feature_params,transition_params)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "calc_featgrad=feature_grad(Y_train,X_train,alphabet,feature_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00790173, -0.00741463, -0.00733574, -0.00708563, -0.00152063,\n",
       "       -0.0214657 , -0.00189108, -0.00608977, -0.00565361, -0.00741145])"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_featgrad[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00790398, -0.00741687, -0.00733822, -0.00708787, -0.00152144,\n",
       "       -0.02146795, -0.00189334, -0.00609199, -0.00565587, -0.00741369])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_gradient[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.49996772796e-09\n",
      "            Iterations: 39\n",
      "            Function evaluations: 54\n",
      "            Gradient evaluations: 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99996418,  0.99992689])"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as optimize\n",
    "def func(x, sign=-1.0):\n",
    "    return sign*(-(1-x[0])**2 - 100*(x[1]-x[0]**2)**2)\n",
    "\n",
    "def func_deriv(x, sign=1.0):\n",
    "    \"\"\" Derivative of objective function \"\"\"\n",
    "    dfdx0 = sign*(2*(1-x[0]) + 400*x[0]*(x[1]-x[0]**2))\n",
    "    dfdx1 = sign*(-200*(x[1]-x[0]**2))\n",
    "    return np.array([ dfdx0, dfdx1 ])\n",
    "\n",
    "res = optimize.minimize(func, [-1.0,1.0], args=(-1.0,), jac=func_deriv,method='SLSQP', options={'disp': True})\n",
    "res.x\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
